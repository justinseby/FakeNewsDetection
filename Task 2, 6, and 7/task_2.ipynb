{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccc112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNCData:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Define class for Fake News Challenge data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_instances, file_bodies):\n",
    "\n",
    "        # Load data\n",
    "        self.instances = self.read(file_instances)\n",
    "        bodies = self.read(file_bodies)\n",
    "        self.heads = {}\n",
    "        self.bodies = {}\n",
    "\n",
    "        # Process instances\n",
    "        for instance in self.instances:\n",
    "            if instance['Headline'] not in self.heads:\n",
    "                head_id = len(self.heads)\n",
    "                self.heads[instance['Headline']] = head_id\n",
    "            instance['Body ID'] = int(instance['Body ID'])\n",
    "\n",
    "        # Process bodies\n",
    "        for body in bodies:\n",
    "            self.bodies[int(body['Body ID'])] = body['articleBody']\n",
    "\n",
    "    def read(self, filename):\n",
    "\n",
    "        \"\"\"\n",
    "        Read Fake News Challenge data from CSV file\n",
    "\n",
    "        Args:\n",
    "            filename: str, filename + extension\n",
    "\n",
    "        Returns:\n",
    "            rows: list, of dict per instance\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialise\n",
    "        rows = []\n",
    "\n",
    "        # Process file\n",
    "        with open(filename, \"r\", encoding='utf-8') as table:\n",
    "            r = DictReader(table)\n",
    "            for line in r:\n",
    "                rows.append(line)\n",
    "\n",
    "        return rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd7ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_train(train, test, lim_unigram):\n",
    "    \"\"\"\n",
    "\n",
    "    Process train set, create relevant vectorizers\n",
    "\n",
    "    Args:\n",
    "        train: FNCData object, train set\n",
    "        test: FNCData object, test set\n",
    "        lim_unigram: int, number of most frequent words to consider\n",
    "\n",
    "    Returns:\n",
    "        train_set: list, of numpy arrays\n",
    "        train_stances: list, of ints\n",
    "        bow_vectorizer: sklearn CountVectorizer\n",
    "        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n",
    "        tfidf_vectorizer: sklearn TfidfVectorizer()\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise\n",
    "    heads = []\n",
    "    heads_track = {}\n",
    "    bodies = []\n",
    "    bodies_track = {}\n",
    "    body_ids = []\n",
    "    id_ref = {}\n",
    "    train_set = []\n",
    "    train_stances = []\n",
    "    cos_track = {}\n",
    "    test_heads = []\n",
    "    test_heads_track = {}\n",
    "    test_bodies = []\n",
    "    test_bodies_track = {}\n",
    "    test_body_ids = []\n",
    "    head_tfidf_track = {}\n",
    "    body_tfidf_track = {}\n",
    "\n",
    "    # Identify unique heads and bodies\n",
    "    for instance in train.instances:\n",
    "        head = instance['Headline']\n",
    "        #print(head)\n",
    "        body_id = instance['Body ID']\n",
    "        #print(body_id)\n",
    "        if head not in heads_track:\n",
    "            heads.append(head)\n",
    "            heads_track[head] = 1\n",
    "        if body_id not in bodies_track:\n",
    "            #print(train.bodies[body_id])\n",
    "            bodies.append(train.bodies[body_id])\n",
    "            bodies_track[body_id] = 1\n",
    "            body_ids.append(body_id)\n",
    "\n",
    "    for instance in test.instances:\n",
    "        head = instance['Headline']\n",
    "        body_id = instance['Body ID']\n",
    "        if head not in test_heads_track:\n",
    "            test_heads.append(head)\n",
    "            test_heads_track[head] = 1\n",
    "        if body_id not in test_bodies_track:\n",
    "            test_bodies.append(test.bodies[body_id])\n",
    "            test_bodies_track[body_id] = 1\n",
    "            test_body_ids.append(body_id)\n",
    "\n",
    "    # Create reference dictionary\n",
    "    for i, elem in enumerate(heads + body_ids):\n",
    "        id_ref[elem] = i\n",
    "    \n",
    "    # Create vectorizers and BOW and TF arrays for train set\n",
    "    bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
    "    bow = bow_vectorizer.fit_transform(heads + bodies)  # Train set only\n",
    "\n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
    "        fit(heads + bodies + test_heads + test_bodies)  # Train and test sets\n",
    "    \n",
    "    # Process train set\n",
    "    for instance in train.instances:\n",
    "        head = instance['Headline']\n",
    "        body_id = instance['Body ID']\n",
    "        head_tf = tfreq[id_ref[head]].reshape(1, -1)\n",
    "        body_tf = tfreq[id_ref[body_id]].reshape(1, -1)\n",
    "        if head not in head_tfidf_track:\n",
    "            head_tfidf = tfidf_vectorizer.transform([head]).toarray()\n",
    "            head_tfidf_track[head] = head_tfidf\n",
    "        else:\n",
    "            head_tfidf = head_tfidf_track[head]\n",
    "        if body_id not in body_tfidf_track:\n",
    "            body_tfidf = tfidf_vectorizer.transform([train.bodies[body_id]]).toarray()\n",
    "            body_tfidf_track[body_id] = body_tfidf\n",
    "        else:\n",
    "            body_tfidf = body_tfidf_track[body_id]\n",
    "        if (head, body_id) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(head, body_id)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(head, body_id)]\n",
    "        feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
    "        train_set.append(feat_vec)\n",
    "        train_stances.append(label_ref[instance['Stance']])\n",
    "\n",
    "    return train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb6b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_test(test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Process test set\n",
    "\n",
    "    Args:\n",
    "        test: FNCData object, test set\n",
    "        bow_vectorizer: sklearn CountVectorizer\n",
    "        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n",
    "        tfidf_vectorizer: sklearn TfidfVectorizer()\n",
    "\n",
    "    Returns:\n",
    "        test_set: list, of numpy arrays\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise\n",
    "    test_set = []\n",
    "    heads_track = {}\n",
    "    bodies_track = {}\n",
    "    cos_track = {}\n",
    "\n",
    "    # Process test set\n",
    "    for instance in test.instances:\n",
    "        head = instance['Headline']\n",
    "        body_id = instance['Body ID']\n",
    "        if head not in heads_track:\n",
    "            head_bow = bow_vectorizer.transform([head]).toarray()\n",
    "            head_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
    "            head_tfidf = tfidf_vectorizer.transform([head]).toarray().reshape(1, -1)\n",
    "            heads_track[head] = (head_tf, head_tfidf)\n",
    "        else:\n",
    "            head_tf = heads_track[head][0]\n",
    "            head_tfidf = heads_track[head][1]\n",
    "        if body_id not in bodies_track:\n",
    "            body_bow = bow_vectorizer.transform([test.bodies[body_id]]).toarray()\n",
    "            body_tf = tfreq_vectorizer.transform(body_bow).toarray()[0].reshape(1, -1)\n",
    "            body_tfidf = tfidf_vectorizer.transform([test.bodies[body_id]]).toarray().reshape(1, -1)\n",
    "            bodies_track[body_id] = (body_tf, body_tfidf)\n",
    "        else:\n",
    "            body_tf = bodies_track[body_id][0]\n",
    "            body_tfidf = bodies_track[body_id][1]\n",
    "        if (head, body_id) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(head, body_id)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(head, body_id)]\n",
    "        feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
    "        test_set.append(feat_vec)\n",
    "\n",
    "    return test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8824e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(pred, file):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Save predictions to CSV file\n",
    "\n",
    "    Args:\n",
    "        pred: numpy array, of numeric predictions\n",
    "        file: str, filename + extension\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file, 'w') as csvfile:\n",
    "        fieldnames = ['Stance']\n",
    "        writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for instance in pred:\n",
    "            writer.writerow({'Stance': label_ref_rev[instance]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2bf4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from csv import DictReader\n",
    "from csv import DictWriter\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt   \n",
    "import seaborn as sns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ffacf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "label_ref = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}\n",
    "label_ref_rev = {0: 'agree', 1: 'disagree', 2: 'discuss', 3: 'unrelated'}\n",
    "stop_words = [\n",
    "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
    "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
    "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
    "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
    "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
    "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
    "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
    "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
    "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
    "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
    "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
    "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
    "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
    "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
    "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
    "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
    "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f394a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names pointing to covid dataset\n",
    "\n",
    "file_train_instances = \"corona_train_stances.csv\"\n",
    "file_train_bodies = \"corona_train_bodies.csv\"\n",
    "file_test_instances = \"corona_test_stances_unlabeled.csv\"\n",
    "file_test_bodies = \"corona_test_bodies.csv\"\n",
    "file_test_instances_labeled = \"corona_test_stances_labeled.csv\"\n",
    "\n",
    "\n",
    "# file names pointing to economic dataset\n",
    "\n",
    "#file_train_instances = \"economic_train_stances.csv\"\n",
    "#file_train_bodies = \"economic_train_bodies.csv\"\n",
    "#file_test_instances = \"economic_test_stances_unlabeled.csv\"\n",
    "#file_test_bodies = \"economic_test_bodies.csv\"\n",
    "#file_test_instances_labeled = \"economic_test_stances_labeled.csv\"\n",
    "\n",
    "\n",
    "# file names pointing to fake news competition dataset\n",
    "\n",
    "#file_train_instances = \"train_stances.csv\"\n",
    "#file_train_bodies = \"train_bodies.csv\"\n",
    "#file_test_instances = \"test_stances_unlabeled.csv\"\n",
    "#file_test_bodies = \"test_bodies.csv\"\n",
    "#file_test_instances_labeled = 'predictions_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdeffb72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.FNCData object at 0x0000021F7E7FBF40>\n",
      "\n",
      "<__main__.FNCData object at 0x0000021F042FEF40>\n",
      "\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "# Load data sets\n",
    "raw_train = FNCData(file_train_instances, file_train_bodies)\n",
    "raw_test = FNCData(file_test_instances, file_test_bodies)\n",
    "n_train = len(raw_train.instances)\n",
    "\n",
    "print(raw_train)\n",
    "print()\n",
    "print(raw_test)\n",
    "print()\n",
    "print(n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e30d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Data Sets\n",
    "lim_unigram = 5000\n",
    "train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer = pipeline_train(raw_train, raw_test, lim_unigram=lim_unigram)\n",
    "feature_size = len(train_set[0])\n",
    "test_set = pipeline_test(raw_test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b123fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_set)\n",
    "X_test = np.array(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f495cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(pd.get_dummies(pd.read_csv(file_train_instances)['Stance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70436447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df23359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 6000\n",
    "# Max number of words in each text.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9e4e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4428497",
   "metadata": {},
   "source": [
    "# Model training for corona and economic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7831044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3/3 [==============================] - 1723s 709s/step - loss: 0.0000e+00 - accuracy: 0.1702 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "batch_size = 50\n",
    "model.fit(X_train,y_train,epochs=epochs,batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51adb8",
   "metadata": {},
   "source": [
    "# Model training for FNC competition datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2be2ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 641s 176s/step - loss: 0.0000e+00 - accuracy: 0.1964 - val_loss: 0.0000e+00 - val_accuracy: 0.1538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1863da08f10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "batch_size = 5000\n",
    "model.fit(X_train,y_train,epochs=epochs,batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b13d30",
   "metadata": {},
   "source": [
    "# Below ecoding part is for y_test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4803e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df):\n",
    "    enc_list = []\n",
    "    for i in df:\n",
    "        if i=='agree':\n",
    "            enc_list.append(0)\n",
    "        elif i=='disagree':\n",
    "            enc_list.append(1)\n",
    "        elif i=='discuss':\n",
    "            enc_list.append(2)\n",
    "        else:\n",
    "            enc_list.append(3)\n",
    "    return enc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb9d4b90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_test_instances_labeled)['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fee92a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(pd.get_dummies(df['Stance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ffee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4ce3c5ab",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8293e",
   "metadata": {},
   "source": [
    "# End of code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
